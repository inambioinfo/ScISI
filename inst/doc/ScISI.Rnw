%
% NOTE -- ONLY EDIT ScISI.Rnw!!!
%
%\VignetteIndexEntry{ScISI Working Paper}
%\VignetteDepends{}
%\VignetteKeywords{Interactome}
%\VignettePackage{ScISI}
\documentclass{article}

\usepackage{hyperref}
\usepackage{natbib}

\textwidth=6.2in
\textheight=8.5in
%\parskip=.3cm
\oddsidemargin=.1in
\evensidemargin=.1in
\headheight=-.3in

\newcommand{\Rfunction}[1]{{\texttt{#1}}}
\newcommand{\Robject}[1]{{\texttt{#1}}}
\newcommand{\Rpackage}[1]{{\textsf{#1}}}
\newcommand{\Rmethod}[1]{{\texttt{#1}}}
\newcommand{\Rfunarg}[1]{{\texttt{#1}}}
\newcommand{\Rclass}[1]{{\textit{#1}}}



\newcommand{\classdef}[1]{%
  {\em #1}
}

\begin{document}
\title{Creating an Silico Interactome}
\maketitle

\section{Introduction}

Understanding protein interactions is relevant to our understanding of
complex biological processes and to deciphering the different roles
played by genes and proteins. In particular developing a set of maps
that associate proteins with multiprotien complexes will be an
essential step in this process. A number of high throughput
experiments have been carried out with the goal of helping to
elucidate these nature and number of their occurances \citep{Gavin,
  Ho, Krogan, Ito, Uetz, Giot}. Yet our knowledge remains
incomplete. Such experiments while valuable are expensive and time
consuming to carry out, suggesting that simulation models may be
beneficially employed to identify the viability of experimental
procedures, to aid in integrating data from disparate sources, and to
provide a sound basis for the development of new methodology in
designing experiments and in analyzing the resultant data. In this
paper we outline one approach in the creation of \textit{in silico
  interactomes} that would be suitable for such simulation
experiments. 

In this paper we consider the problem of estimating an \textit{in
  silico} interactome, ISI, for \textit{Saccharomyces cerevisiae} and
we describe five different roles where such a computational device can
be used.  First, the ISI provides a tool that can be used to mimic
wet-lab experiments; these \textit{in silico} experiments can help to
confirm the viability of the wet-lab experimental procedures by
allowing investigators to alter experimental conditions (by the modification
of both stochastic and systematic error rates) and then measure how
the outputs change as the error rates do. Second, the ISI provides a
tool from which multiple data sets can be generated, under different conditions,
so that the statistical properties of different proposed estimation
procedures can be directly compared. A third use for the ISI is to use
it to help develop tools and strategies for small scale experiments
that can probe the interactome at a very detailed level. For example,
a reasonable strategy to investigate a coherent set of protein
complexes is to first select a small number of candidates and explore
their interactions, say via affinity purification (co-precipitation)
experiments. Once the first experiment has been collected one would
like to analyze the data and determine an optimal, or nearly optimal
set of baits for future experiments. A fourth use of an ISI is to
study the effect of perturbations in a network.  Such perturbations
could be caused by evolution (and hence could become part of the study
of evolutionary pressure on the interactome) or due to drug treatments
(and hence would become part of a clinical investigation into the
likely effects of drug treatments). The fifth use for such a tool is
to explore and understand the effects of different sampling paradigms
on the inferences that can drawn from different wet-lab
experiments. We note that there are a number of recent papers
\citep{Oltavi,Goverview, SubsampNetworks} that raise many important
issues in this regard.

An ISI is not without its limitiations. It cannot reveal new
relationships between its members, but rather can only reflect the
current state of biological knowledge. It can, however, be used in any
of the five tasks listed above and the outputs of the computational
experiments will often provide predictions of real interactions that
could be tested. Further, one type of interaction can be used either
to predict or to model other types of interactions, and provide
information about the likelihood of interactions; the verification of
interactions remains the domain of experimental biology.

Protein interactions are very dynamic by nature and that different
interactions occur at different times and under different
conditions. For example, some protein complexes such as those needed
to facilitate cell division are often transient in nature appearing
when needed and dissovling when no longer required, where as other
complexes such as the ribosome (and its subunits) are essentially
permanently constituted.  We provide some tools that will allow
investigators the ability to indicate situations and conditions under
which an interaction is known to occur and we discuss a paradigm where
an investigator will be able to select all interactions for a given
condition. 

While the estimated interactome is itself of some great interest, we
note that it is more important to describe the procedure by which an
ISI is produced. There are many reasons why the estimate is of less
importance, among them the fact that the methodology can be applied to
many organisms and tissue types to yield specialized ISI estimates. We
also note that the data used in the construction is constantly being
updated and improved, hence users will want to regularly update any
estimate as new data sources become availble, or old ones are
updated. Hence, to be viable there must be a mechanism for
updating and refining the ISI. We also note that there are many
different and disparate data sources and the system should be
sufficiently flexible to allow users to select data sources
appropriate for their studies as well as the ability to incorporate
additional data from new and potentially different experimental
techniques. Finally we note that investigators will often want to
modify the ISI, either with locally produced, though not yet public,
data, or to satisfy personal beliefs. The paradigm we describe here
allows investigators to create their own personalized estimates, to
share interactomes and to revise and update interactomes.

In this paper we first define the notion of a biological interactome
and give various examples of such interactomes. We then present a set
of computational tools, embodied in the \Rpackage{ScISI} package, that
we have assembled to construct the \textit{in silico interactome}. The
general steps that must be carried out are, first assembling the data
that are to be used for the construction and second to process those
data into an estimated interactome that can be used as an input for
the various computational tasks described above. Finally we report on
some \textit{in silico} experiments that can be performed using the
estimated interactome.


\section{Methodology}

In our discussion we do not distinguish between the term gene and the
term protein, since the available resolution is not fine enough and
equating these concepts leads to a more comprehensible dialog. The
methodology is sufficiently general to be able to address the more
complicated situation of different protein/polypeptide variants should
they arise.

\subsection{Biological Interactomes}

In the broadest sense, an interactome is a set of elements $P$, for
our purposes $P$ is the set of genes, or their related proteins,
within a particular organism or cell type. The elements of $P$ have a
variety relationships with each other and each of these different
relationships can be modeled using some form of graph. Some
relationships are binary, for example, protein $p_1$ is known to
directly physically interact with some other protein, $p_2$, perhaps
under a specific set of conditions. In other cases, the relations are
not binary but rather are one to many or many to many. The example we
use for this second type of relationship is that of protein complex
membership. In this setting all members of a complex are related to
each other, but they need not directly interact and the relationship
is not binary. A reasonable model for such data is as either a
hypergraph \cite{Berge} or, equivalently, the bipartite graph
representation. Proteins are grouped into sets according to whether
they are in a complex and these sets constitute the hyperedges of the
hypergraph. There is no need for the hyperedges to be disjoint and
proteins can be in many or few hyperedges, depending on whether the
protein is in many or few complexes.

We begin by distinguishing between many different types of
interactomes with a few general comments, we will make the
terms and ideas more explicit in later sections of this report. There
are interactomes that model direct binary interactions and hence are
related to experiments using the yeast two hybrid (Y2H) system
\citep{Y2H, Ito, Uetz}. A second type of interactome is one that
models protein complex membership. In this model, two proteins are
said to interact if they are constituent members of some protein complex 
under the conditions of interest. This second type of interactome is
more closely related to co-precipitation or affinity-purification
experiments \citep{Ho, Gavin, Krogan}. We note that proteins which
interact in the second sense, need not interact in the first sense;
hence, we separate these two different, but related, concepts. Other
interactomes can be created by considering other relationships such as
those between transcription factors and their direct targets or the
interaction network that can be defined on pairs of synthetically
lethal genes \citep{Tong2004}.

We can put these concepts into a more biological framework as
follows. Let $P$ denote the expressed genome of some cell or tissue,
and hence each such gene is represented by a node in our graph. Any
two genes that have either a direct biochemical or biophysical
interaction will have an edge between them. We remark that a gene may
be associated with itself if it induces a homodimer and such an
observation would be represented by a self-loop in the graph. 
Next, we consider a different set of relationships which are those of
protein complex co-membership. In this, different graph, we again
represent the genes as nodes, but now edges are determined by whether
or not the two proteins are found as  constituent parts of some
protein complex. If represented in this way there is a strict loss of
information, since a graph can only represent binary relationships yet
protein complex membership is a many-to-one relationship. For this
reason we prefer the bipartite graph since in that format no
information is lost.

These two biological interactomes are related, but they are also
distinct, and the use of the data in modeling and other downstream
uses requires that some caution be used to not confuse the
concepts. It is certainly the case that for our working definition of
a protein complex the members of that complex will have some binary
interactions with other members. But not all members have direct
physical interactions with other members, so we would not use a
so-called \textit{matrix} model \cite{FIXME:matrixmodelcitation} to
represent those interactions. These two examples highlight the subtle
differences between two distinct interactomes and underscores the
necessity to keeping distinct interactomes separate. We will refer to
the first of these interactomes as $I_b$, where the $b$ subscript
reinforces the notion that binary relationships are being modeled and
the second as $I_c$ where the $c$ subscript reinforces the notion that
protein complexes are being modeled.

There are of course many limitations to such an approach, however,
there will be ample opportunity to refine and revise the models
proposed here as our knowledge of the underlying biology improves and
as the available computer technology improves. Among the limitations
is the observation that some interactions will only take place under a
specific set of conditions, and that other interactions, sometimes
involving the same proteins, will occur under other conditions. Thus,
the model representing these data will need to become richer and to be
able to indicate under which conditions a given relationship is known
to occur. Some interactions are specific to certain tissues (in higher
organisms) or to particular phases of the cell cycle, and so on. The
models we propose can be extended in fairly straightforward ways to
encompass these details, but the available data are currently to
sparse to warrant such an approach. We will note some experiments, and
some uses of existing experimental data, that can provide some
insight. 

In this paper, we will report on a set of methodological tools that
can be used to construct an \textit{in silico interactome}
for \textit{S. cerevisiae}. We restrict our attention to the two
interactomes described above, $I_b$ and $I_c$. As we noted above, not
all complexes are present at all times in the cells under study, but
we are unable to model this aspect of the data due to the lack of
appropriate data on which to base such classifications. When such data
become more widely available they can easily be accomodated within the
paradigm we are proposing.

We first define what we mean by a protein complex and describe
different models for representing these data. Next we give a concise
description of the computational tool \textit{ScISI} and show its
dynamic properties in constructing different complex membership
interactomes. We then describe the process of creating an \textit{in
  silico} protein complex interactome via \textit{ScISI} based on two
public meta-data sources and three experimental data sets by combining
purification of protein complexes with identification of their
individual components via mass spectroscopy. Finally we consider the
construction of an \textit{in silico} binary interactome based
primarily on data from IntAct, which is itself based on high
throughput experimental data (FIXME: list the yeast data here). 

\subsection{Protein complexes and their representations}

%%FIXME: a few pictures would surely help readers - how are we making
%%out on that?

We define a protein complex to be two or more proteins that physically
interact for the purposes of carrying out some biological
objective. There is no need for all members of a complex to physically
interact with each one another, but all the proteins do need to
combine to form a connected multi-protein structure. Some proteins
will be involved in many complexes while others will be involved in
relatively few. Protein complexes range in size from two proteins to
more than fifty.

There are sure to be situations where a protein complex, that exists
under one set of conditions, is a proper subset of a complex that
exists under another set of conditions. For some analyses it will be
important to remove those complexes which are proper subsets of other
complexes and we provide tools to help do that.

%FIXME: it would be good to give some sort of concrete
%example of what might go wrong.


\subsection{Graphs and Hypergraphs}

We require a small amount of graph theory to allow for succinct
discussion of the relevant concepts. A graph $G$, consists of a pair
of sets; the vertices, or nodes, $V$, and the edges, or relations,
$E$. We write $G=(V,E)$. Graphs can be used to represent binary
relationships and hence are an appropriate model and data structure
for $I_b$. They cannot represent the more complicated situation of
modeling protein complex co-membership. In this case we require the
use of hypergraphs \cite{Berge}. A hypergraph, $H$, consists of a set
of nodes, $V$, and a set of hyperedges. A hyperedge is any subset of
$V$. Thus, for modeling protein interaction data the vertex set is the
set of proteins in the organism or tissue under study. To represent binary
relationships we will use a graph, where the edges indicate the
presence of a binary interaction and for complex co-membership data we
will use a hypergraph, where the hyperedges are the identities of the
proteins that constitute the complex. There is a one-to-one
relationship between hypergraphs and bipartite graphs and in some
reports bipartite graphs are used.

There are a variety of different representations for both graphs and
hypergraphs. We will mainly make use of the incidence matrix
representation for $H$ and the adjacency matrix representation of $G$.
For the protein complex hypergraph the
incidence matrix representation, $M_B$, is a $\{0,1\}$-matrix where
proteins index the rows and protein complexes index the columns. The
$(i,j)$ entry for $M_B$ is one if protein $p_i$ is a constituent
member of complex $c_j$; the entry is zero otherwise. When studying
binary interactions the representation will be of a graph, and then
the adjacency matrix is typically square (or upper triangular) with
the proteins indexing both the rows and the columns. In this matrix a
one in position $(i,j)$ indicates that protein $p_i$ interacts with
protein $p_j$. This matrix is likely to be sparse and we will make use
of appropriate software tools and data representations.

The construction of the \textit{in silico} interactomes, $I_c$ and
$I_b$, will be the construction of appropriate graph representations
of these entities and for the most part we will compute adjacency
matrices to represent the data.

\subsection{Constructing the Interactome}

We first discuss the creation, or estimation of $I_c$. Our basic
strategy is to assemble a number of software tools, together with
available data, from databases or experimental results. We process the
data from each database or experiment separately and then sequentially
integrate the different protein complex estimates into an estimate.
Some investigators may want to restrict their attention to well-known
and documented complexes while others will be interested in exploring
likely complex co-memberships. For this reason we construct two
estimates of $I_C$ in yeast. One is based only on complexes reported
in GO and MIPS \citep{GO, GOA, MIPS}, while the other extends this first
estimate to include data from high throughput co-precipitation
experiments. 

We remark that the complexes documented within GO and MIP are
based on many different technologies such as small scale protein
complex verification experiments. Many of these protein complexes have
been exhaustively verified.

The choice of input data sources and the manner in which they are
processed is subjective and different investigators are likely to make
different decisions. We report the sources that we used and the
decisions made; others can easily make use of alternative or
additional sources. We make use of two rather distinct data sources,
one is the set of published and annotated protein complex data that
can be obtained from the Gene Ontology (GO), \citep{GO, GOA},
(\url{www.geneontology.org}) and the Munich Information Center for
Protein Sequences (MIPS). Additionally we will make use of protein
complex estimates from three publicly available wet-lab experiments
\citep{Ho, Gavin, Krogan}. Rather than make use of these data
directly, we first apply the technology of \cite{Scholtens,
  ScholtensVidalGent} since, as demonstrated in those references, the
resultant data are more consistent with known biological complexes.

%%Overall approach: these sources have textual descriptions of
%% sets of genes and in some cases the sets of genes represent complexes
%% in the absence of a data source provided set of complexes we search
%% the terms for keywords, we also carried out different hand curations
%% to include other terms that had not been picked up by this approach

\subsubsection{Keyword searching}

For each data repository the textual terms used to describe its
contents are searched for specific key words. We make use of regular
expressions searches and approximate matching regular expression
searches. By default all terms that
contain one of the three expressions listed here,
\begin{enumerate}
\item  the exact word \texttt{complex};
\item one or more words that end with the suffix \texttt{ase};
\item one or more words that end with the suffix \texttt{some}.
\end{enumerate}

In particular we use the following three regular expression,
\verb+complex+, \verb+\\Base\\b+, and \verb+\\Bsome\\b+ in our search.
These narrow the pool of return values yet include terms such as
\texttt{DNA-directed Polymerase II, holoenzyme} or 
\texttt{repairosome}. 

Users can modify the search criteria to add new terms or to remove any
of those default values listed above. Terms selected correspond to
protein complexes and these are then collected and the corresponding
incidence matrix is computed. The rows are indexed by the standard
gene names and the columns are indexed by the protein complex identification 
codes (unique to each different repository). 


%%FIXME: name names or drop this
While most protein interaction repositories store protein complex data
by listing the constituent proteins for each complex, there are a
small number of repositories which store the bipartite graph incidence
graph matrix for the protein complex data. For these repositories,
we simply download these matrices.

%\subsection{Processing Online Repositories}
%%FIXME: I don't think it is the right name - 
%% one distinction is those repositories where there are 
%% sets of genes with text descriptions, only some sets are complexes
%% and we need to extract them. Other repositories have protein
%% complex predictions - then we don't need searching. Are there
%% examples of the second type?

Both GO and MIPS provide additional information on the curation and
provenance of the data that they report. This information comes in the
form of evidence codes, which are used by both resource, although the
codes themselves are unique to the different resources. Users can
specify a list of evidence codes and disallow those proteins whose
annotations are found in the supplied list. Within the GO
repository, we disallow those proteins annotated by the following
evidence codes: Inferred from Electronic Annotation (IEA);
Non-traceable Author Statement (NAS); No Biological Data Available
(ND); Not Recorded(NR). 
%%FIXME: please put both the GO and the MIPS exclusions into tables (two
%% tables one for each). And then write some text that refers to them.
For the MIPS repository, we restrict the
following: Overview Information - review and text (901.01.03,
901.01.03.01, 901.01.03.02); Personal Communication - via homepage
(web) or electronic mail (901.01.04, 901.01.04.01, 901.01.04.02), and
Closed Information - institution or private (901.01.05, 901.01.05.01,
901.01.05.02). In addition to the MIPS evidence codes listed above, we have
dis-allowed proteins which are indexed exclusively by the following
codes:

"902.01.01.02.01.01.02" (co-immunoprecipitation, epitope tag)
"902.01.01.04.01.03" (matrix-assisted laser desorption/ionization time-of-light
mass spectrometry (MALDI TOF MS))
"902.01.09.02" (high throughput experiment)

%%FIXME: for some of these evidence codes we remove the predicted
%%complexes because they are based on data that we will later perform
%%our own modeling. I am not sure whether they should be included in
%%the ScISIverified or not?

When the function is given these three evidence codes, all high
through-put protein complexes are eliminated (Gavin, Ho, Krogan) with
the exception of one Krogan protein complex. This unique Krogan
protein complex (originally with 14 constituent proteins) has 2
proteins which are indexed by some other evidence codes in addition to
those listed above, and therefore, the function selected these two
proteins and the protein complex for which they belong in its parse
mechanism. Because we have built \Rpackage{ScISI} in a way so that
users can allow or dis-allow proteins based on evidence codes, this
small anomoly cannot be circumvented by the function itself. We shall
describe in a later section on QC for minor anomolies.


%%FIXME: the numbers in here should be part of an \Sexpr and be
%%computed from the actual data - otherwise why are we using a
%%vignette? 
We made use of the version in the Bioconductor package \Rpackage{GO}
version 1.10.0, which is version number XXX. We found 208 protein
complexes. The MIPS repository (as of 17 December 2005) yielded 141
protein complexes. The GO protein complexes include 1,213 unique
proteins while the MIPS protein complexes contains 651 unique
proteins. 

As mentioned before some of the MIPS protein complexes have been
uniquely identified by high through-put experimentation such as
affinity purification - mass spectroscopy ; we have, however,
deliberately chosen not to extract these protein
complexes. %%FIXME: then we should do the same for GO? I don't know how, do you Denise?
The protein complexes were estimated by clustering algorithms
determined by the individual investigator of the high through-put
experiments, and therefore, protein complexes identified via high
through-put AP-MS technologies lack uniformity and consistency from
one experiment to the next. We have chosen one standard algorithm to
parse high through-put data and estimate protein complex composition
from the raw data by this algorithm.
%%FIXME: I would like to say that they tend to be too big - in that
%%they include proteins that we would not use - maybe we need to give
%%some specific examples - RNA Pol I, II and III? By being too big we
%%then have some problems down the road, as estimates from apComplex
%%will appear to be subcomplexes and get removed.

\subsection{APMS Experimental Data}

We additionally obtained the reported results from three high
throughput APMS experiments \citep{Ho, Gavin, Krogan}. These data were
assembled and processed using the technology reported in
\cite{Scholtens, ScholtensVidalGent}, in particular using the software
packages \Rpackage{apComplex} available from the Bioconductor project.

%%FIXME: Denise, can you add a descriptive paragraph or two.

The output of the analysis are putative protein complexes. They come
in several different varieties, and we make use exclusively of the
multi-bait multi-hit (MBMH) complex estimates. Again, these form a
hypergraph and we can obtain separate incidence matrix representations
for the three experiments.

\subsection{Analysis and Merging of the Data-Sets}

Table~\ref{ta:Repetitions} reports the number of protein complexes 
that were redundant by comparing the repositories
pairwise including self comparisons. Table~\ref{ta:Repetitions} shows 
that the only data-set with self-redundancy is derived from the 
MIPS repository; and upon investigation, MIPS has categorized protein
complexes by known functionality, and three protein complexes were 
placed in two different categories each. Table~\ref{ta:Repetitions} 
directly shows that the no two repositories has extremely large 
overlap.

<<testStuff, echo=FALSE, results=hide>>=
library("ScISI")
data("redundantM")
data("subCompM")
@ 


<<redundant, eval = TRUE, echo=FALSE, results=tex >>=


xtable(redundantM, display = c("s","d","d","d","d","d"), label =
"ta:Repetitions", caption="Number of repetitive Protein Complexes") 

@ 

Not limited to redundant protein complexes, we also process each
data-set independently to find and to remove all protein
sub-complexes. The results are reported in Table~\ref{ta:subComplexes}
where the row names indicate the location of the sub-complex and the
column names indicate the location of the aggregate protein complex.
Reading across row two of Table~\ref{ta:subComplexes} we see that GO
contains protein complexes that were protein sub-complexes in each of
the other data repositories including
itself. Table~\ref{ta:subComplexes} is much less sparse relative to
Table~\ref{ta:Repetitions}, and it is this relative comparison that
makes all the estimates more credible since each data repository is
based on unique experiments coupled with unique estimation algorithms.

<<redundant, eval = TRUE, echo=FALSE, results=tex>>=

xtable(subCompM, display = c("s","d","d","d","d","d"), label =
"ta:subComplexes", caption = "Number of Protein Sub-Complexes") 
@ 

Merging the multiple sub-interactomes into one is done 
iteratively by combining the estimates pairwise.

Let $I_{GO}$ and $I_{MIPS}$ be two estimates that we wish to merge. We
define the merging of two interactomes as not only combining the
protein complexes from $I_{GO}$ to $I_{MIPS}$, but also removing
protein complexes which are common to both interactomes. In addition
to removing redundant protein complexes, we have removed any protein
complex of $I_{GO}$ that is a sub-complex of a complex in $I_{MIPS}$
and vice versa.

Once the estimates $I_{GO}$ and $I_{MIPS}$ are merged into $I_{2}$,
say, we can repeat the process described above by merging $I_{2}$ with
another estimate. This process is repeated until all estimates are
merged. We will refer to the resultant estimated interactome as
$I_c$. %%FIXME: is it worth pointing out/proving that order does not matter?

%%The stuff written below does not seem to fit in the methods section,
%%but more in a discussion section??? Seems like the intro to a dynamic
%%algorithm paper or something???

%When estimates of the interactome are available from different sources
%we must devise a method for combining them to provide a larger and
%more general estimate. How we do this depends a bit on the estimates
%themselves. We leave to another time a discussion of how to address
%issues of systematic and stochastic errors and rather consider the
%problem of having two different estimates of an interactome that we
%want to combine into a non-redundant joint estimate. The two different
%estimates may come from different sources and hence could overlap by a
%little or a lot. 

%The so-called \textit{alignment} problem arises because some
%complexes might not be labeled or are  unambiguously identified. The genes (or
%proteins) that make them up, are well identified but the collections
%are not. So, one of the first steps is to identify redundancy and to
%eliminate it. For simplicity we presume that the complexes in one
%estimate are labeled $C_1, \ldots, C_k$ and in the other
%$K_1,\ldots,K_l$, where $k$ and $l$ are generally not the same.
%We will discuss set theoretic relationships between the $C_i$ and
%$K_j$ in terms of the proteins that constitute the complexes. Thus,
%the statement, $C_i = K_j$, means that all proteins in $C_i$ are in
%$K_j$ and the converse also holds. The statement, $C_i \subset K_j$
%indicates that $C_i$ is a proper subset of $K_j$, that is, they are
%not equal.

%%FIXME: could we look at the intersection of their respective PPI
%%graphs as a measure of concordance?

%Complete concordance occurs when $C_i = K_j$, such complexes are
%reasonably easy to identify and only one of them is retained. A
%somewhat more difficult case is when $C_i \subset K_j$. For some
%technologies we will retain $C_i$ and for others we want to retain
%$K_j$. The user will have to specify which of the two alternatives
%they want. Both cannot be retained due to identifiability
%problems. %%FIXME: is this true - and do you have an example etc?

%Since much of the data is derived from experimentation and the
%underlying experiments tend to have high error rates (FIXME:citations
%etc here) some consideration of near concordance must also be
%considered. 

%What measures will we use? What graphics/summary statistics can be produced?


\section{The estimate}

%%FIXME: these numbers should be computed from the package, not given
The result of combining data from multiple database sources is an aggregate 
protein complex membership \textit{in silico interactome}. The elements of 
this interactome consists of $1755$ proteins and $743$ protein complexes. The 
number of proteins is consistent with biological estimates (%%~40% of genome?)
of the number of expressed genes for the a particular genome;
the number of unique protein complexes gives us enough complexity to 
conduct \textit{in silico} experiments in order to emulate true wet-lab
experiments.


Histogram for the cardinalities or protein complexes?
Histogram for the order of each particular protein's complex membership?

What else should we put here???
   
\section{Validation}

There are a number of different methods that can be used to validate
the ScISI prediction. Our basic premise is that the complex is a
collection of physically connected proteins. While, in some cases this
may not be strictly true and there could be two or more subcomponents
that do not physically interact we will demonstrate that such cases
have little impact on the validation methods we employ and hence are,
to a large extent, covered by our proposals.

If all physical interactions between proteins were known, then the
problem would be somewhat straightforward to address. One would simply
take a predicted complex and determine whether the constituent
elements interact and deem the complex \textit{viable} if it could be
configured as a single connected component. However, very few
interactions are known, and it is this incompleteness that requires us
to make inference about whether the prediction is likely to be
valid. Unfortunately the topology of the complexes (the set of actual
true interactions) is unknown and hence cannot itself be used to
assess the observed data. We do not know how many edges (physical
interactions) there should be, and whether the complex follows a spoke
topology, a matrix topology or any of the many other
possibilities. 

We also note that one of the alternatives that some have used, which
is to compare their predictions to the published, and to some extent
verified complexes described in GO and MIPS is not available to us. We
used those sources in construction of our estimate and hence cannot
validate against them. Instead we make use of two other sources of
data. One being the set of observed yeast two-hybrid (Y2H) experiments
available from IntAct and the other being a set of predicted
protein-protein interactions from \cite{PPIpred}.

Before describing the data we briefly discuss the statistical issues
involved. In most cases only a subset of the genes were assayed, or
considered and it will be important to restrict the computations to 

A working hypothesis is that every protein complex is a single
connected component, so that when the complex is functional it is a
single unit. This does not mean that every protein physically
interacts with every other protein, in some cases a protein may
interact with only one other protein, while in other cases it may
interact with many. %%FIXME: Tony we need some pictures, we should
                    %%have some small number of proteins with labeled
                    %%edges so we can refer to them in the text.

\subsection{Y2H Validation}

%%FIXME: Tony, please report here 1) number of experiments, that we
%%are dropping those with less than 20 interactions since there is
%%likely to be substantial experiment to experiment variability.
%% Maybe a table with experiments, system (Gal4), number of baits
%% and whether it is genome wide.

We rely on Y2H data provided by IntAct (citation) and compute a number
of summary statistics for the complex estimates in the ScISI. We
obtained data on 41 Y2H experiments comprising 1980 unique reported bait 
proteins and 3073 unique reported prey proteins. It is important to 
recognize the fact that information can
only be obtained for complex estimates that contain one or more Y2H
bait proteins. And since the actually physical interactions are not
known, the topology of the Y2H graph is also not known.
%%FIXME: Tony describe the number of experiments here, a table
%%describing something about baits into

Any bait protein in a Y2H experiment should find one or more complex
co-members provided those co-members were available as prey in the Y2H
experiment. This proviso indicates one of the true weaknesses of Y2H
data as they are often reported since only information about the prey
that were detected and not the prey that were tested is
reported. Thus, the absence of an edge is not as informative as it
could be. We do not know if it was tested and not found or not tested,
and these are very different things.


\subsection{PPI Predictions}

<<ppipred, echo=FALSE, results=hide>>=
library("y2hStat")
data(ppipred)
@ 

\cite{PPIpred} report \Sexpr{nrow(ppipred)} pairwise predictions
between proteins in \textit{S. cerevisiae}, with a presumed 
false positive rate of $3E-4$ and a presumed false negative rate of
$0.85$. The predictions are accompanied by a probability and one can
select only those predicted interactions according to this
probability. We have chosen to use only those interactions where the
probability is at least $0.5$. This greatly reduces the number of 
predicted interactions and also greatly reduces the number of
proteins that were observed. 

When using these data, only proteins that are predicted to be involved
in at least one interaction are \textit{observable}. This changes as
the probability cut-off changes and can be interpreted as the
sensitivity of the analysis. This phenomenon is no different than that
observed with different experimental procedures, not all interactions can
be detected with all procedures and one must make some efforts to determine
which interactions can be detected.

Once the observable proteins have been determined they can be use to
compute the different summary statistics discussed in
Section~\ref{sec:summary}. 

For each protein complex, $C$, we divide its constituent proteins into two
sets, those that are observable, denoted $U$ and those that are not,
$\overline{U} = C \\ U$. If $P \in U$, then we expect there to be 
an edge between $P$ and some other member of $C$, but we will only
observe this edge if it is to another member of $U$. 


\subsection{Summary Statistics}
\label{sec:summary}

As stated above, our definition of a multiprotein complex presumes
that every member is physically linked to at least one other member of
the complex. Stated in graph theoretic terms we assume that the graph
of a complex where nodes represent proteins and edges represent
physical interactions is connected. 

To develop appropriate statistical methods we describe the same
setting in a slightly more abstract notation. For a given complex,
$C$, say, consisting of $n$ proteins the graph that represents this
complex must be connected, by our definition of a complex. Hence,
there are between $n-1$ and $n(n-1)/2$ edges. We can conceptualize
this in the following way. Consider an urn that contains balls, each
ball represents on possible edge, or binary interaction. For $C$ there
are $n(n-1)/2$ possible edges and hence there are that many balls in the
urn. Some of the balls represent edges that truly exist in $C$, and these
are colored black, the remainder of the balls are white.

We will, in some cases, use the notation $G_C = (V_C, E_C)$, to denote
the graph induced by $C$. We note that in this graph the edges are not
directed and there are no self-loops. That is, we assume that no
protein in $C$ has an edge to itself, in part this makes the math
simpler and in part it reflects the technology. While AP-MS can
determine the constituent elements of complex it cannot directly
ascertain their multiplicity.

The first problem we address is estimating the number of black balls
in the urn. We label this unknown quantity $X$. The basis for this
estimation is the sampling of some nodes of $C$ and determining which
other members of $C$ they are observed to be connected to. We will
presume that the nodes that are sampled are a simple random sample
from the population $V_C$, but note that this is not always the case
in real experiments. Let $k$ denote the number of nodes sampled and
let $n= |V_C|$, when needed we will also use $n_C$. Further, let $x$
denote the number of distinct edges found based on the sampling of $k$
nodes. 

To determine how many edges were tested we note that the first node is
compared to the remaining $n-1$, the second to the remaining $n-2$ and
so on. So, the number of edges tested is $[(n-1)+(n-2)+\ldots+(n-k)]$.
If we sample all nodes then the sequence is, 
\[
\sum_{j=1}^{n-1} (n-j) = \sum_{j=1}^{n-1} j = \frac{(n-1)n}{2},
\]
where we have made use of the well known relationship regarding the sum
of the first $n$ integers. And hence, sampling all nodes does in fact
result in the inspection of all edges.

A widely used estimate of $X$ arises from equating the observed
proportion of edges in the sample, with the unknown proportion in the
population. That is,
\begin{eqnarray*}
\frac{\hat{X}}{n(n-1)/2} & = & \frac{x}{[(n-1)+(n-2)+\ldots+(n-k)]} \\
\hat{X} & = & \frac{ x (n)(n-1)}{2 [(n-1)+(n-2)+\ldots+(n-k)]} \\
\end{eqnarray*}

A second estimator can be derived from the following argument. If the
nodes that are sampled represent a random sample from the population
of nodes, $V_C$, then the observed mean degree is an unbiased estimate
of the population mean degree. The population mean degree times $n$
divided by 2 is an estimate of the number of edges. For each sampled
node we let $d_i$ denote its observed degree. So, we have
\[
\tilde{X} = \frac{ n \sum_{i=1}^k d_i}{2k}.
\]

The two estimators are quite similar, and sometimes identical.

At this point we have merely estimated the number of edges in $G_C$,
from that we make the next step of assessing whether or not $G_C$ is
connected. There are a number of factors that influence that
determination and we provide simulation examples and evidence of how
these different factors can be used.

First, the larger $X$ the more likely the graph is connected. Next, we
can examine the number of unique nodes that were selected either as
part of the $k$ nodes used to probe the graph or as the other ends of
the detected edges. The more nodes detected this way, the higher the
probability that the graph is connected. And finally, we can take the
observed subgraph, induced by our sampling, and determine how many
more edges are needed to obtain a connected graph; the fewer the more
likely it is that the true graph is connected.

We also explore a simulation approach to assessing whether the
underlying graph is connected. Given the observed edges for the $k$
query nodes and the estimated number of edges in the graph, $X$, one
can simulate the remainder of the graph by drawing from the remaining
edges. For each sample, one can assess whether the graph is
connected. Repeating this a large number of times yields some number
of connected graphs and some number of unconnected graphs. The
relative proportions can be used to assign a probability that the
underlying graph is connected. This approach can also be loosened if
desired and other measures made on the simulated graphs.

% If the graph is connected there must be at least $n-1$ black balls,
% but simply having $n-1$ black balls will not ensure that the graph is
% connected. However, the more black balls there are, the more likely it
% is that the graph is connected. We use this observation as the basis
% for our proposals. We let $X$ represent the true number of black balls
% in the urn and note that this number is unknown and hence needs to be
% estimated.
%%FIXME: note that if we associate a pair of names with each ball then
%%the graph is connected when the union of those names is of size n,
%%but it is not obvious to me how we might use that.

% Both sampling schemes, Y2H and PPI predictions, can be viewed as
% drawing some number of balls from the urn. For each ball drawn we do
% know if that ball represents a true edge (and hence is black) or not,
% provided we are willing to presume that there are no false positives
% and no false negatives. Thus the observed data can be interpreted as
% having $k$ black balls among $l$ draws. A simple estimate of $X$ can
% then be found by equating the proportion of black balls in the sample
% with the proportion of black balls in the population. And hence that, 
% \[
% \hat{X} = \frac{k n (n-1)}{l},
% \]
% and one can also easily show that $E(k n(n-1) / l) = X$, under a
% random sampling scheme and that
% $var(k n(n-1)/l ) = X(n(n-1) - X) (n(n-1) - l) / [l (n(n-1) - 1)]$,
% although one suspects that better estimates can be obtained.

% A potentially better idea is to use the fact that for some types of sampling,
% especially that of Y2H, the observed out-degree is an unbiased
% estimate of the true outdegree. That, times the number of nodes/genes
% in the complex gives an estimate of the number of edges, which has
% the advantage of being unbiased. One suspects that better estimates
% of the number of edges could be acheived by using in-degree as well,
% but those estimates are biased.

Once we address real experimental data, the situation changes and
becomes more problematic. Some alternative approaches will be needed. 
In addition we propose three different summary statistics and
investigate their behavior using the ScISI and the available Y2H data.

\begin{enumerate}
\item For a given protein complex, $C_i$, find all complex members,
  $P_j$ that were used as a bait in some Y2H experiment. For these,
  compute the proportion that found at least one other member of the
  complex. If $P_j$ was used as a bait in more than one Y2H experiment
  do not double count, but take any positive result as positive.
\item For a given protein complex, $C_i$, find the average out-degree
  of all bait proteins, again avoid double counting, in this case by
  taking the maximum out-degree. Divide this by the complex size.
\item Given the number of proteins in a complex that are detected as
  either bait or prey, find the number that are connected to at least
  one other complex co-member. [Should this be the proportion of
  complex members that are connected to at least one other complex
  member?] 
\item Compute the ratio of the number of edges needed to make the
  complex connected, by adding to the observed edges, divided by the
  minimum number of edges needed to create a connected graph from the
  complex. This is essentially a measure of incompleteness.
\end{enumerate}


\section{Discussion}

What did we learn,

\bibliographystyle{plainnat}
\bibliography{bioc}

\end{document}
